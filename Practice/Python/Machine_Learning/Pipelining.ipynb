{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70252ad0",
   "metadata": {},
   "source": [
    "# `Pipelining`:\n",
    "In ml, a pipeline is a sequence of data processing steps that are chained together to automated and streamline the ml workflow. A pipeline allows you to combined multiple data preprocessing and model training steps into a single object, making it easier to organize and manage your ml code.\n",
    "\n",
    "1. preprocessing\n",
    "2. model training\n",
    "3. model evaluation\n",
    "4. prediction\n",
    "\n",
    "# `Advantage`\n",
    "1. Simplified workflow\n",
    "2. Avoiding Data Leakage\n",
    "3. Streamlined modal deployment\n",
    "4. Hyperparameter Tuning\n",
    "\n",
    "`model contain or information retain in pipeline`\n",
    "\n",
    "# `Summary`:\n",
    "\n",
    "Overall, pipeline are a powerful tool for managing and automating the machine learning workflow, promoting code reusability, consistency, efficiency. They help streamline the development and deployment of ml models, making it easier to iterate and expierment with different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4037c3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the titanic datasetfrom seaborn \n",
    "titanaic_data= sns.load_dataset('titanic')\n",
    "\n",
    "# Select features and target variables\n",
    "X= titanaic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y= titanaic_data['survived']\n",
    "\n",
    "# Select the data into train and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the column transformer for imputing missing values\n",
    "numeric_features=['age', 'fare']\n",
    "categorical_features= ['pclass', 'sex', 'embarked']\n",
    "\n",
    "numeric_transformer= Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer= Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor= ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with the preprocessor and RandomForestClassifier\n",
    "pipeline= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction on the test data\n",
    "y_pred= pipeline.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy= accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33723b98",
   "metadata": {},
   "source": [
    "# `Hyperparameter Tuning in Pipeline:`\n",
    " in a pipeline involves optimizing the hyperparameters of the different steps in the pipeline to find the best combination that maximizes the model's performance. Here's an example of hyperparameter tuning in a pipeline and selecting the best modell ont he titanic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b901f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8212290502793296\n",
      "Best Hyperparameters: {'model__max_depth': None, 'model__min_samples_split': 5, 'model__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load the titanic datasetfrom seaborn \n",
    "titanaic_data= sns.load_dataset('titanic')\n",
    "\n",
    "# Select features and target variables\n",
    "X= titanaic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y= titanaic_data['survived']\n",
    "\n",
    "# Select the data into train and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline= Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune (diff param in same model we tune and deal the things)\n",
    "hyperparameters = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 5, 10],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# perform GridSearchCV:\n",
    "grid_search= GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model= grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred= best_model.predict(X_test)\n",
    "\n",
    "# Calculate the best hyperperameters\n",
    "accuracy= accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print('Best Hyperparameters:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c22db",
   "metadata": {},
   "source": [
    "# Select the best model in Pipeline:\n",
    "To select the best model when using multiple models in a pipeline, you can use techniques like cross-validation and evaluation metrics to compare their performance. Here's an example of how to accomplish this on the titanic datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02cbead1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Cross Validation Accuracy: 0.7991529597163399\n",
      "Test Accuracy: 0.8379888268156425\n",
      "\n",
      "Model: Gradient Boost\n",
      "Cross Validation Accuracy: 0.8061952132374668\n",
      "Test Accuracy: 0.7988826815642458\n",
      "\n",
      "Model: XGBoost\n",
      "Cross Validation Accuracy: 0.8034177090515119\n",
      "Test Accuracy: 0.7932960893854749\n",
      "\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', RandomForestClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the feature and target\n",
    "X = df_titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boost', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance \n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the performance metrics\n",
    "    print('Model:', name)\n",
    "    print('Cross Validation Accuracy:', mean_accuracy)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = pipeline\n",
    "\n",
    "# Retrieve the best model\n",
    "print('Best Model:', best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39fc7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Cross Validation Accuracy: 0.7991529597163399\n",
      "Test Accuracy: 0.8379888268156425\n",
      "\n",
      "Model: Gradient Boost\n",
      "Cross Validation Accuracy: 0.8061952132374668\n",
      "Test Accuracy: 0.7988826815642458\n",
      "\n",
      "Model: XGBoost\n",
      "Cross Validation Accuracy: 0.8034177090515119\n",
      "Test Accuracy: 0.7932960893854749\n",
      "\n",
      "Model: Decision tree\n",
      "Cross Validation Accuracy: 0.7865655471289273\n",
      "Test Accuracy: 0.8268156424581006\n",
      "\n",
      "Model: SVC\n",
      "Cross Validation Accuracy: 0.8160248202501723\n",
      "Test Accuracy: 0.8044692737430168\n",
      "\n",
      "Model: Linear Regression\n",
      "Cross Validation Accuracy: 0.7977839062346105\n",
      "Test Accuracy: 0.8100558659217877\n",
      "\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', RandomForestClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select the feature and target\n",
    "X = df_titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = df_titanic['survived']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boost', GradientBoostingClassifier(random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(random_state=42)),\n",
    "    ('Decision tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('SVC', SVC(random_state=42)),\n",
    "    ('Linear Regression', LogisticRegression(random_state=42))\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate over the models and evaluate their performance \n",
    "for name, model in models:\n",
    "    # Create a pipeline for each model\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "    # Calculate mean accuracy\n",
    "    mean_accuracy = scores.mean()\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the performance metrics\n",
    "    print('Model:', name)\n",
    "    print('Cross Validation Accuracy:', mean_accuracy)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    print()\n",
    "\n",
    "    # Check if the current model has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = pipeline\n",
    "\n",
    "# Retrieve the best model\n",
    "print('Best Model:', best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf777bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ydata_profiling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
